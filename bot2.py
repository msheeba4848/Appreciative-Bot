import streamlit as st
import ollama
import os
import time

# ğŸŒŸ Ensure Ollama is Running (Lazy Load)
def check_ollama():
    """Verifies if Ollama is running before making API calls."""
    try:
        ollama.chat(model="mistral:latest", messages=[{"role": "system", "content": "Hello"}])
    except Exception as e:
        st.error(f"ğŸ’” Ollama Connection Error: {str(e)}. Run `ollama serve` in your terminal.")
        st.stop()

check_ollama()  # Ensure Ollama is running before using it

# ğŸ’¬ AI Chatbot Response (Optimized for Faster Execution)
def chatbot_response(user_message):
    """Fetch AI response from Ollama with minimal delay."""
    try:
        start_time = time.time()  # Measure response time
        response = ollama.chat(
            model="mistral:latest",
            messages=[{"role": "user", "content": user_message}]
        )
        end_time = time.time()
        print(f"ğŸ”¹ Ollama Response Time: {end_time - start_time:.2f} seconds")
        return response["message"]["content"]
    except Exception as e:
        return f"ğŸ’” Error: {str(e)}"

# ğŸ­ Streamlit UI
st.title("ğŸ’Œ AI Love Bot")
mode = st.radio("ğŸ’– Choose Mode", ["ğŸ’Œ Send a Love Email", "ğŸ’¬ Chat with AI Love Assistant"])

# ğŸ’¬ AI Love Assistant
if mode == "ğŸ’¬ Chat with AI Love Assistant":
    st.subheader("ğŸ’¬ AI Love Assistant (Fast Mode)")
    user_input = st.text_input("ğŸ’Œ **Enter your romantic message:**")

    if user_input:
        with st.spinner("ğŸ’– AI Love Bot is thinking..."):
            response = chatbot_response(user_input)
        st.markdown(f"ğŸ’¬ **AI Love Bot:** {response} ğŸ’–")
